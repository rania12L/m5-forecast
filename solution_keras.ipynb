{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = \"m5-forecasting-accuracy\"\n",
    "\n",
    "calendar = pd.read_csv(os.path.join(path_input, \"calendar.csv\"))\n",
    "selling_prices = pd.read_csv(os.path.join(path_input, \"sell_prices.csv\"))\n",
    "sample_submission = pd.read_csv(os.path.join(path_input, \"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "calendar = calendar.drop([\"date\", \"weekday\"], axis=1)\n",
    "calendar = calendar.assign(d = calendar.d.str[2:].astype(int))\n",
    "calendar = calendar.fillna(\"missing\")\n",
    "cols = list(set(calendar.columns) - {\"wm_yr_wk\", \"d\"})\n",
    "calendar[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(calendar[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = selling_prices.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n",
    "selling_prices[\"sell_price_rel_diff\"] = gr.pct_change()\n",
    "selling_prices[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n",
    "selling_prices[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) / (1 + gr.cummax() - gr.cummin())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, var in enumerate([\"state_id\", \"store_id\", \"cat_id\", \"dept_id\"]):\n",
    "    plt.figure()\n",
    "    g = sns.countplot(sales[var])\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "    g.set_title(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.item_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_sales(df, drop_d = None):\n",
    "    if drop_d is not None:\n",
    "        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n",
    "    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n",
    "    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n",
    "    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "                 var_name='d', value_name='demand')\n",
    "    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n",
    "    return df\n",
    "\n",
    "sales = reshape_sales(sales, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['lag_t28'] = sales.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n",
    "sales['rolling_mean_t30'] = sales.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "sales['rolling_mean_t60'] = sales.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(60).mean())\n",
    "sales['rolling_mean_t90'] = sales.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "sales['rolling_std_t30'] = sales.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "\n",
    "sales = sales[(sales.d >= 1914) | (pd.notna(sales.rolling_mean_t180))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.merge(calendar, how=\"left\", on=\"d\")\n",
    "gc.collect()\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\n",
    "sales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "gc.collect()\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\n",
    "cat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n",
    "                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "\n",
    "# In loop to minimize memory use\n",
    "for i, v in tqdm(enumerate(cat_id_cols)):\n",
    "    sales[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales[[v]])\n",
    "\n",
    "sales.head()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n",
    "            \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t30\", \"rolling_mean_t60\", \n",
    "            \"rolling_mean_t90\", \"rolling_mean_t180\", \"rolling_std_t7\", \"rolling_std_t30\"]\n",
    "bool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\n",
    "dense_cols = num_cols + bool_cols\n",
    "\n",
    "# Need to do column by column due to memory constraints\n",
    "for i, v in tqdm(enumerate(num_cols)):\n",
    "    sales[v] = sales[v].fillna(sales[v].median())\n",
    "    \n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sales[sales.d >= 1914]\n",
    "test = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n",
    "                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\n",
    "test.head()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X(df):\n",
    "    X = {\"dense1\": df[dense_cols].to_numpy()}\n",
    "    for i, v in enumerate(cat_cols):\n",
    "        X[v] = df[[v]].to_numpy()\n",
    "    return X\n",
    "\n",
    "X_test = make_X(test)\n",
    "\n",
    "flag = (sales.d < 1914) & (sales.d >= 1914 - 28)\n",
    "valid = (make_X(sales[flag]),\n",
    "         sales[\"demand\"][flag])\n",
    "\n",
    "flag = sales.d < 1914 - 28\n",
    "X_train = make_X(sales[flag])\n",
    "y_train = sales[\"demand\"][flag]\n",
    "                             \n",
    "del sales, flag\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lr=0.001):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    dense_input = Input(shape=(len(dense_cols), ), name='dense1')\n",
    "\n",
    "    wday_input = Input(shape=(1,), name='wday')\n",
    "    month_input = Input(shape=(1,), name='month')\n",
    "    year_input = Input(shape=(1,), name='year')\n",
    "    event_name_1_input = Input(shape=(1,), name='event_name_1')\n",
    "    event_type_1_input = Input(shape=(1,), name='event_type_1')\n",
    "    event_name_2_input = Input(shape=(1,), name='event_name_2')\n",
    "    event_type_2_input = Input(shape=(1,), name='event_type_2')\n",
    "    item_id_input = Input(shape=(1,), name='item_id')\n",
    "    dept_id_input = Input(shape=(1,), name='dept_id')\n",
    "    store_id_input = Input(shape=(1,), name='store_id')\n",
    "    cat_id_input = Input(shape=(1,), name='cat_id')\n",
    "    state_id_input = Input(shape=(1,), name='state_id')\n",
    "\n",
    "    wday_emb = Flatten()(Embedding(7, 1)(wday_input))\n",
    "    month_emb = Flatten()(Embedding(12, 1)(month_input))\n",
    "    year_emb = Flatten()(Embedding(6, 1)(year_input))\n",
    "    event_name_1_emb = Flatten()(Embedding(31, 1)(event_name_1_input))\n",
    "    event_type_1_emb = Flatten()(Embedding(5, 1)(event_type_1_input))\n",
    "    event_name_2_emb = Flatten()(Embedding(5, 1)(event_name_2_input))\n",
    "    event_type_2_emb = Flatten()(Embedding(5, 1)(event_type_2_input))\n",
    "\n",
    "    item_id_emb = Flatten()(Embedding(3049, 3)(item_id_input))\n",
    "    dept_id_emb = Flatten()(Embedding(7, 1)(dept_id_input))\n",
    "    store_id_emb = Flatten()(Embedding(10, 1)(store_id_input))\n",
    "    cat_id_emb = Flatten()(Embedding(3, 1)(cat_id_input))\n",
    "    state_id_emb = Flatten()(Embedding(3, 1)(state_id_input))\n",
    "\n",
    "    x = concatenate([dense_input, wday_emb, month_emb, year_emb, \n",
    "                     event_name_1_emb, event_type_1_emb, \n",
    "                     event_name_2_emb, event_type_2_emb, \n",
    "                     item_id_emb, dept_id_emb, store_id_emb,\n",
    "                     cat_id_emb, state_id_emb])\n",
    "    x = Dense(150, activation=\"tanh\")(x)\n",
    "    x = Dense(75, activation=\"tanh\")(x)\n",
    "    x = Dense(10, activation=\"tanh\")(x)\n",
    "    outputs = Dense(1, activation=\"linear\", name='output')(x)\n",
    "\n",
    "    inputs = {\"dense1\": dense_input, \"wday\": wday_input, \"month\": month_input, \"year\": year_input, \n",
    "              \"event_name_1\": event_name_1_input, \"event_type_1\": event_type_1_input,\n",
    "              \"event_name_2\": event_name_2_input, \"event_type_2\": event_type_2_input,\n",
    "              \"item_id\": item_id_input, \"dept_id\": dept_id_input, \"store_id\": store_id_input, \n",
    "              \"cat_id\": cat_id_input, \"state_id\": state_id_input}\n",
    "\n",
    "    # Connect input and output\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    model.compile(loss=keras.losses.mean_squared_error,\n",
    "                  metrics=[\"mse\"],\n",
    "                  optimizer=keras.optimizers.RMSprop(learning_rate=lr))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, 'model_keras.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, \n",
    "                    y_train,\n",
    "                    batch_size=10000,\n",
    "                    epochs=50,\n",
    "                    shuffle=True,\n",
    "                    validation_data=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"demand\"] = pred.clip(0)\n",
    "submission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[sample_submission.columns]\n",
    "submission = sample_submission[[\"id\"]].merge(submission, how=\"left\", on=\"id\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
